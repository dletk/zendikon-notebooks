{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zendikon Package\r\n",
        "Ensure that the `zendikon` package is available in: \r\n",
        "\r\n",
        "1. The current environment (notebook) we are currently in.\r\n",
        "2. The pipeline's environment once it is submitted to AML."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Figure out if we want to use this method going forward\n",
        "# Use this wheel URL in conda_dependencies.yml\n",
        "whl_url = Environment.add_private_pip_wheel(workspace, \"./zendikon-1.8.8.post12-py3-none-any.whl\", exist_ok=True)\n",
        "whl_url"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "'https://zendikontest6824921913.blob.core.windows.net/azureml/Environment/azureml-private-packages/zendikon-1.8.8.post12-py3-none-any.whl'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441177667
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Setup\r\n",
        "\r\n",
        "We begin by using the AML SDK to establish the AML workspace, experiment and compute target we will be utilizing. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace, Experiment, ComputeTarget, Dataset, Environment\n",
        "\n",
        "# AML Setup\n",
        "workspace = Workspace.from_config()\n",
        "print('Workspace name: ' + workspace.name,\n",
        "      'Azure region: ' + workspace.location,\n",
        "      'Subscription id: ' + workspace.subscription_id,\n",
        "      'Resource group: ' + workspace.resource_group, sep='\\n')\n",
        "\n",
        "experiment_name = \"reusable_pipeline_time_series_forecasting\"\n",
        "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
        "compute_target = ComputeTarget(workspace=workspace, name=\"zendikon-cpu-f4\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: zendikon-test\nAzure region: eastus2\nSubscription id: 6f83e421-6b03-4154-9df4-fc8739806b66\nResource group: zendikon\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441171427
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing dataset for pipeline\r\n",
        "Registered datasets in AML are used as input datasets to Zendikon pipelines. We can achieve this in several ways:\r\n",
        "\r\n",
        "1. Use [AML Studio (UI)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-connect-data-ui?tabs=credential) to use an existing external datastore and register datasets from it.\r\n",
        "2. The same can be achieved with the [AML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets).\r\n",
        "\r\n",
        "However for simplicity, we directly upload and register a small dataset included with this example with the SDK.\r\n",
        "Only do this step if you decide to register the dataset via code instead of using the UI."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = os.path.join(\"data\")\n",
        "\n",
        "input_df = pd.read_csv(os.path.join(data_dir, \"beer_dataset.csv\"), parse_dates=True)\n",
        "\n",
        "datastore = workspace.get_default_datastore()\n",
        "\n",
        "# Register and upload the entire beer dataset to the workspace\n",
        "input_dataset = Dataset.Tabular.register_pandas_dataframe(input_df, datastore, \"beer_input_dataset\", show_progress=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/14208358-220a-4952-9845-11857f1edb2f/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441177094
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Zendikon Reusable Pipeline\r\n",
        "\r\n",
        "A Zendikon pipeline consists of individual steps that each process some input datasets to produce output datasets.\r\n",
        "These steps can be either be reusable ones provided by the Zendikon library, or fully custom ones specified by the user.\r\n",
        "\r\n",
        "In this example, we show a minimal example of mixing simple custom pre-processing steps and the provided step for leveraging AML's AutoML for modeling."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare the custom pipeline steps\r\n",
        "\r\n",
        "Each custom step's logic is specified in its own Python script. We extend `BasePipelineStep` with information about each step's script location and use these classes for declaring the pipeline later on.\r\n",
        "\r\n",
        "Zendikon provides function decorators that simplify the definition of custom steps. The scripts in `./step_scripts/*.py` provide more concrete examples of how these decorators are used."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.step.base_step import BasePipelineStep, StepConfig\n",
        "from zendikon.pipelines.pipeline import PipelineStepInfo\n",
        "\n",
        "\n",
        "class DataSplittingStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig) -> None:\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"split_data.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n",
        "\n",
        "class PreprocessStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig):\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"preprocess.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441177831
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all steps together into a pipeline\r\n",
        "\r\n",
        "Here, we declare the flow of our pipeline by declaring steps along with their associated information:\r\n",
        "- Input datasets: Can either be the name of 1) a registered dataset in the AML workspace, or 2) an output from a prior step.\r\n",
        "- Output datasets: Names of the output datasets, corresponding to the step function's outputs.\r\n",
        "- CLI arguments: Argument values to pass to `StepArgument`s of the step function.\r\n",
        "- Execution environment: Dependencies required for the step to run.\r\n",
        "\r\n",
        "In this case, besides our custom steps above, we can also simply utilize Zendikon-provided ones such as `ZendikonAutoMLStep` and `ZendikonAutoMLInferenceStep` to leverage AutoML."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.reusable_steps.automl_step import ZendikonAutoMLStep\n",
        "from zendikon.pipelines.reusable_steps.automl_inference_step import ZendikonAutoMLInferenceStep\n",
        "\n",
        "steps_info = [\n",
        "    PipelineStepInfo(PreprocessStep, \n",
        "        StepConfig(\"Select relevant columns\", inputs=[\"beer_input_dataset\"], outputs=[\"beer_processed_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(DataSplittingStep, \n",
        "        StepConfig(\"Splitting data to train and validation\", inputs=[\"beer_processed_dataset\"], outputs=[\"beer_train_dataset\", \"beer_valid_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLStep, \n",
        "        StepConfig(\"AutoML training\", inputs=[\"beer_train_dataset\"], outputs=[\"models_info\", \"best_model\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLInferenceStep, \n",
        "        StepConfig(\"AutoML inferencing with best model\", inputs=[\"beer_valid_dataset\", \"best_model\"], outputs=[\"best_model_predicted\"], arguments={\"target_column\": \"BeerProduction\"}, conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441491589
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the forecasting parameters to be used in the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "target_column_name = \"BeerProduction\"\n",
        "time_column_name = \"DATE\"\n",
        "forecast_horizon = 12\n",
        "freq = \"MS\"\n",
        "\n",
        "# Forecasting Parameters\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name=time_column_name,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    freq=freq,  # Set the forecast frequency to be monthly (start of the month)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441177360
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline instance\r\n",
        "\r\n",
        "primary_metric=“NormRMSE” (normalized root mean squared error, by default). In order to change the primary metric, specify the parameter primary_metric when calling TimeSeriesForecastingPipeline.from_default_settings.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.time_series.forecasting import TimeSeriesForecastingPipeline\n",
        "\n",
        "pipeline = TimeSeriesForecastingPipeline.from_default_automl_config(\n",
        "    input_datasets=[],\n",
        "    forecasting_parameters=forecasting_parameters,\n",
        "    label_column_name=target_column_name,\n",
        "    compute_targets=[compute_target],\n",
        "    steps_info=steps_info)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nWarning: The following datasets (['beer_input_dataset']) are not the output(s) of any step in your pipeline and are assumed to be registered in your pipeline workspace.\nIf this is not your intention, we recommend interrupting the command using Ctrl + c and check your pipeline config.\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441492833
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit Pipeline\r\n",
        "\r\n",
        "The pipeline will now execute remotely on our specified compute target, and we can track the progress in AML Studio with the generated link below:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.submit(experiment, wait_for_completion=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step Select relevant columns [79a5e91c][0f8aa93b-3cd7-4b7f-996f-e7a170ea6bc5], (This step is eligible to reuse a previous run's output)Created step Splitting data to train and validation [30bdccc9][71c7a0e8-2c3b-499f-87f6-fc4fefa5a086], (This step is eligible to reuse a previous run's output)\n\nCreated step AutoML training [05efd96a][72ec4c8d-7669-4cfe-a190-d39f713bd3ed], (This step will run and generate new outputs)\nCreated step AutoML inferencing with best model [bab490de][1ec7a7fb-d154-486d-9dee-a2dd2d566df3], (This step will run and generate new outputs)\nSubmitted PipelineRun 6a5fe01c-6c92-44cd-953c-23e5b166775a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/6a5fe01c-6c92-44cd-953c-23e5b166775a?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "Run(Experiment: reusable_pipeline_time_series_forecasting,\nId: 6a5fe01c-6c92-44cd-953c-23e5b166775a,\nType: azureml.PipelineRun,\nStatus: Preparing)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>reusable_pipeline_time_series_forecasting</td><td>6a5fe01c-6c92-44cd-953c-23e5b166775a</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/6a5fe01c-6c92-44cd-953c-23e5b166775a?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1647441500559
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get results"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load our dataset\n",
        "\n",
        "In this example, we will be using an small example dataset which contains information about beer production across time.  \n",
        "To demonstrate using custom steps, we will first do some simple pre-processing of this dataset.  \n",
        "Finally, we will leverage [AML's AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml) feature via a Zendikon re-usable step to perform the basic modeling for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>grain</th>\n",
              "      <th>BeerProduction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1992-01-01</td>\n",
              "      <td>grain</td>\n",
              "      <td>3459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1992-02-01</td>\n",
              "      <td>grain</td>\n",
              "      <td>3458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1992-03-01</td>\n",
              "      <td>grain</td>\n",
              "      <td>4002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1992-04-01</td>\n",
              "      <td>grain</td>\n",
              "      <td>4564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1992-05-01</td>\n",
              "      <td>grain</td>\n",
              "      <td>4221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         DATE  grain  BeerProduction\n",
              "0  1992-01-01  grain            3459\n",
              "1  1992-02-01  grain            3458\n",
              "2  1992-03-01  grain            4002\n",
              "3  1992-04-01  grain            4564\n",
              "4  1992-05-01  grain            4221"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = os.path.join(\"~/data\")\n",
        "\n",
        "input_df = pd.read_csv(os.path.join(data_dir, \"beer_dataset.csv\"), parse_dates=True)\n",
        "\n",
        "input_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a Zendikon Reusable Pipeline\n",
        "\n",
        "A Zendikon pipeline consists of individual steps that each process some input datasets to produce output datasets.  \n",
        "These steps can be either be reusable ones provided by the Zendikon library, or fully custom ones specified by the user.\n",
        "\n",
        "In this example, we show a minimal example of mixing simple custom pre-processing steps and the provided step for leveraging AML's AutoML for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Declare the custom pipeline steps\n",
        "\n",
        "Each custom step's logic is specified in its own Python script. We extend `BasePipelineStep` with information about each step's script location and use these classes for declaring the pipeline later on.\n",
        "\n",
        "Zendikon provides function decorators that simplify the definition of custom steps. The scripts in `./step_scripts/*.py` provide more concrete examples of how these decorators are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1647441177831
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from zendikon.pipelines.step.base_step import BasePipelineStep, StepConfig\n",
        "from zendikon.pipelines.pipeline import PipelineStepInfo\n",
        "\n",
        "\n",
        "class DataSplittingStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig) -> None:\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"split_data.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n",
        "\n",
        "class PreprocessStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig):\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"preprocess.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Check our custom steps\n",
        "\n",
        "We can check the functionality of our custom steps by importing the functions and executing them locally as regular Python functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>BeerProduction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1992-01-01</td>\n",
              "      <td>3459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1992-02-01</td>\n",
              "      <td>3458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1992-03-01</td>\n",
              "      <td>4002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1992-04-01</td>\n",
              "      <td>4564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1992-05-01</td>\n",
              "      <td>4221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         DATE  BeerProduction\n",
              "0  1992-01-01            3459\n",
              "1  1992-02-01            3458\n",
              "2  1992-03-01            4002\n",
              "3  1992-04-01            4564\n",
              "4  1992-05-01            4221"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "source_dir = \"./step_scripts\"\n",
        "if source_dir not in sys.path:\n",
        "    sys.path.append(source_dir)\n",
        "\n",
        "from argparse import Namespace\n",
        "from preprocess import preprocess_data\n",
        "from split_data import split_data\n",
        "\n",
        "\n",
        "proc_data = preprocess_data(input_df, cli_args=Namespace(time_column_name=\"DATE\", target_column_name=\"BeerProduction\"))\n",
        "\n",
        "train_data, valid_data = split_data(proc_data, cli_args=Namespace(time_column_name=\"DATE\", split_date=\"2012-01-01\"))\n",
        "\n",
        "train_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Putting all steps together into a pipeline\n",
        "\n",
        "Here, we declare the flow of our pipeline by declaring steps along with their associated information:\n",
        "- Input datasets: Can either be the name of 1) a registered dataset in the AML workspace, or 2) an output from a prior step.\n",
        "- Output datasets: Names of the output datasets, corresponding to the step function's outputs.\n",
        "- CLI arguments: Argument values to pass to `StepArgument`s of the step function.\n",
        "- Execution environment: Dependencies required for the step to run.\n",
        "\n",
        "In this case, besides our custom steps above, we can also simply utilize Zendikon-provided ones such as `ZendikonAutoMLStep` and `ZendikonAutoMLInferenceStep` to leverage AutoML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1647441491589
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f28b3aa0d90>,\n",
              " <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f28b3997c70>,\n",
              " <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f28b33eaf40>,\n",
              " <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f28b33eac70>]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from zendikon.pipelines.reusable_steps.automl_step import ZendikonAutoMLStep\n",
        "from zendikon.pipelines.reusable_steps.automl_inference_step import ZendikonAutoMLInferenceStep\n",
        "\n",
        "steps_info = [\n",
        "    PipelineStepInfo(PreprocessStep, \n",
        "        StepConfig(\"Select relevant columns\", inputs=[\"beer_input_dataset\"], outputs=[\"beer_processed_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(DataSplittingStep, \n",
        "        StepConfig(\"Splitting data to train and validation\", inputs=[\"beer_processed_dataset\"], outputs=[\"beer_train_dataset\", \"beer_valid_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLStep, \n",
        "        StepConfig(\"AutoML training\", inputs=[\"beer_train_dataset\"], outputs=[\"models_info\", \"best_model\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLInferenceStep, \n",
        "        StepConfig(\"AutoML inferencing with best model\", inputs=[\"beer_valid_dataset\", \"best_model\"], outputs=[\"best_model_predicted\"], arguments={\"target_column\": \"BeerProduction\"}, conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "]\n",
        "\n",
        "steps_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Running our pipeline on AML\n",
        "\n",
        "From this point on, we will require an AML workspace to work with.\n",
        "For more details on creating an AML workspace, see [Quickstart: Create workspace resources you need to get started with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources)  \n",
        "\n",
        "Replace the default workspace details below with the workspace you have access to. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Experiment Setup\n",
        "\n",
        "We begin by using the AML SDK to establish the AML workspace, experiment and compute target we will be utilizing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1647441171427
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: zendikon-test\n",
            "Azure region: eastus2\n",
            "Subscription id: 6f83e421-6b03-4154-9df4-fc8739806b66\n",
            "Resource group: zendikon\n",
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Workspace, Experiment, ComputeTarget, Dataset, Environment\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Replace these with your own AML workspace details\n",
        "subscription_id = \"6f83e421-6b03-4154-9df4-fc8739806b66\"\n",
        "resource_group = \"zendikon\"\n",
        "workspace_name = \"zendikon-test\"\n",
        "\n",
        "# Choose a name for your CPU cluster, existing ones can be used.\n",
        "cpu_cluster_name = \"cpu-cluster\"\n",
        "\n",
        "# AML Setup\n",
        "workspace = Workspace(\n",
        "      subscription_id=subscription_id,\n",
        "      resource_group=resource_group,\n",
        "      workspace_name=workspace_name\n",
        ")\n",
        "print('Workspace name: ' + workspace.name,\n",
        "      'Azure region: ' + workspace.location,\n",
        "      'Subscription id: ' + workspace.subscription_id,\n",
        "      'Resource group: ' + workspace.resource_group, sep='\\n')\n",
        "\n",
        "experiment_name = \"reusable_pipeline_time_series_forecasting\"\n",
        "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
        "\n",
        "# Verify that the cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=workspace, name=cpu_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4, \n",
        "                                                           idle_seconds_before_scaledown=2400)\n",
        "    compute_target = ComputeTarget.create(workspace, cpu_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Zendikon Package\n",
        "Ensure that the `zendikon` package is available in: \n",
        "\n",
        "1. The current environment (notebook) we are currently in. If using from JupyterLab, kernel `zendikon-env` should already have this set up.\n",
        "2. The pipeline's environment once it is submitted to AML. To do so, execute the following, copy the blob storage link and update the dependency link by pasting it into `conda_dependencies.yml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1647441177667
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://zendikontest6824921913.blob.core.windows.net/azureml/Environment/azureml-private-packages/zendikon-1.10.0.post1-py3-none-any.whl'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use this wheel URL in conda_dependencies.yml\n",
        "whl_url = Environment.add_private_pip_wheel(workspace, \"./zendikon-1.10.0.post5-py3-none-any.whl\", exist_ok=True)\n",
        "whl_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Preparing dataset for pipeline\n",
        "Registered datasets in AML are used as input datasets to Zendikon pipelines. We can achieve this in several ways:\n",
        "\n",
        "1. Use [AML Studio (UI)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-connect-data-ui?tabs=credential) to use an existing external datastore and register datasets from it.\n",
        "2. The same can be achieved with the [AML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets).\n",
        "\n",
        "However for simplicity, we directly upload and register the small dataset we have loaded in this example with the SDK.\n",
        "Only do this step if you decide to register the dataset via code instead of using the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1647441177094
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating arguments.\n",
            "Arguments validated.\n",
            "Successfully obtained datastore reference and path.\n",
            "Uploading file to managed-dataset/79e41687-94e6-4052-bab3-cd425e37409b/\n",
            "Successfully uploaded file to datastore.\n",
            "Creating and registering a new dataset.\n",
            "Successfully created and registered a new dataset.\n"
          ]
        }
      ],
      "source": [
        "datastore = workspace.get_default_datastore()\n",
        "\n",
        "# Register and upload the entire beer dataset to the workspace\n",
        "input_dataset = Dataset.Tabular.register_pandas_dataframe(input_df, datastore, \"beer_input_dataset\", show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prepare the forecasting parameters to be used in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1647441177360
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "target_column_name = \"BeerProduction\"\n",
        "time_column_name = \"DATE\"\n",
        "forecast_horizon = 12\n",
        "freq = \"MS\"\n",
        "\n",
        "# Forecasting Parameters\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name=time_column_name,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    freq=freq,  # Set the forecast frequency to be monthly (start of the month)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create the pipeline instance\n",
        "\n",
        "primary_metric=“NormRMSE” (normalized root mean squared error, by default). In order to change the primary metric, specify the parameter primary_metric when calling TimeSeriesForecastingPipeline.from_default_settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1647441492833
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Warning: The following datasets (['beer_input_dataset']) are not the output(s) of any step in your pipeline and are assumed to be registered in your pipeline workspace.\n",
            "If this is not your intention, we recommend interrupting the command using Ctrl + c and check your pipeline config.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from zendikon.pipelines.time_series.forecasting import TimeSeriesForecastingPipeline\n",
        "\n",
        "pipeline = TimeSeriesForecastingPipeline.from_default_automl_config(\n",
        "    input_datasets=[],\n",
        "    forecasting_parameters=forecasting_parameters,\n",
        "    label_column_name=target_column_name,\n",
        "    compute_targets=[compute_target],\n",
        "    steps_info=steps_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Submit Pipeline\n",
        "\n",
        "The pipeline will now execute remotely on our specified compute target, and we can track the progress in AML Studio with the generated link below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1647441500559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step Select relevant columns [e196d078][a4c607c5-4545-4d44-b3e9-c3152cfcf51b], (This step is eligible to reuse a previous run's output)Created step Splitting data to train and validation [d65ff2f7][0231ead2-7c52-40ed-803c-025fddebeee6], (This step is eligible to reuse a previous run's output)\n",
            "\n",
            "Created step AutoML training [eeb69ce7][96b7f15c-0dee-4114-b8c2-eb4d358b2f07], (This step is eligible to reuse a previous run's output)\n",
            "Created step AutoML inferencing with best model [03787184][10877721-e95a-4c1b-8d1c-4fff46efe022], (This step is eligible to reuse a previous run's output)\n",
            "Submitted PipelineRun 1459b958-aca0-486e-9015-ad0edfed1c28\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/1459b958-aca0-486e-9015-ad0edfed1c28?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>reusable_pipeline_time_series_forecasting</td><td>1459b958-aca0-486e-9015-ad0edfed1c28</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/1459b958-aca0-486e-9015-ad0edfed1c28?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: reusable_pipeline_time_series_forecasting,\n",
              "Id: 1459b958-aca0-486e-9015-ad0edfed1c28,\n",
              "Type: azureml.PipelineRun,\n",
              "Status: Preparing)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.submit(experiment, wait_for_completion=False)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "zendikon-env",
      "language": "python",
      "name": "zendikon-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

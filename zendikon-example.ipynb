{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load our dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "data_dir = os.path.join(\"~/data\")\r\n",
        "\r\n",
        "input_df = pd.read_csv(os.path.join(data_dir, \"beer_dataset.csv\"), parse_dates=True)\r\n",
        "\r\n",
        "input_df.head()\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "         DATE  grain  BeerProduction\n0  1992-01-01  grain            3459\n1  1992-02-01  grain            3458\n2  1992-03-01  grain            4002\n3  1992-04-01  grain            4564\n4  1992-05-01  grain            4221",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DATE</th>\n      <th>grain</th>\n      <th>BeerProduction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>grain</td>\n      <td>3459</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-02-01</td>\n      <td>grain</td>\n      <td>3458</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-03-01</td>\n      <td>grain</td>\n      <td>4002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-04-01</td>\n      <td>grain</td>\n      <td>4564</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-05-01</td>\n      <td>grain</td>\n      <td>4221</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Zendikon Reusable Pipeline\n",
        "\n",
        "A Zendikon pipeline consists of individual steps that each process some input datasets to produce output datasets.\n",
        "These steps can be either be reusable ones provided by the Zendikon library, or fully custom ones specified by the user.\n",
        "\n",
        "In this example, we show a minimal example of mixing simple custom pre-processing steps and the provided step for leveraging AML's AutoML for modeling."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare the custom pipeline steps\n",
        "\n",
        "Each custom step's logic is specified in its own Python script. We extend `BasePipelineStep` with information about each step's script location and use these classes for declaring the pipeline later on.\n",
        "\n",
        "Zendikon provides function decorators that simplify the definition of custom steps. The scripts in `./step_scripts/*.py` provide more concrete examples of how these decorators are used."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.step.base_step import BasePipelineStep, StepConfig\n",
        "from zendikon.pipelines.pipeline import PipelineStepInfo\n",
        "\n",
        "\n",
        "class DataSplittingStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig) -> None:\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"split_data.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n",
        "\n",
        "class PreprocessStep(BasePipelineStep):\n",
        "    def __init__(self, step_config: StepConfig):\n",
        "        source_directory = Path(\"./step_scripts\")\n",
        "        script_name = \"preprocess.py\"\n",
        "        super().__init__(step_config, source_directory, script_name=script_name)\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1647441177831
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check our custom steps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "source_dir = \"./step_scripts\"\r\n",
        "if source_dir not in sys.path:\r\n",
        "    sys.path.append(source_dir)\r\n",
        "\r\n",
        "from argparse import Namespace\r\n",
        "from preprocess import preprocess_data\r\n",
        "from split_data import split_data\r\n",
        "\r\n",
        "\r\n",
        "proc_data = preprocess_data(input_df, cli_args=Namespace(time_column_name=\"DATE\", target_column_name=\"BeerProduction\"))\r\n",
        "\r\n",
        "train_data, valid_data = split_data(proc_data, cli_args=Namespace(time_column_name=\"DATE\", split_date=\"2012-01-01\"))\r\n",
        "\r\n",
        "train_data.head()\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "         DATE  BeerProduction\n0  1992-01-01            3459\n1  1992-02-01            3458\n2  1992-03-01            4002\n3  1992-04-01            4564\n4  1992-05-01            4221",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DATE</th>\n      <th>BeerProduction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>3459</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-02-01</td>\n      <td>3458</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-03-01</td>\n      <td>4002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-04-01</td>\n      <td>4564</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-05-01</td>\n      <td>4221</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all steps together into a pipeline\n",
        "\n",
        "Here, we declare the flow of our pipeline by declaring steps along with their associated information:\n",
        "- Input datasets: Can either be the name of 1) a registered dataset in the AML workspace, or 2) an output from a prior step.\n",
        "- Output datasets: Names of the output datasets, corresponding to the step function's outputs.\n",
        "- CLI arguments: Argument values to pass to `StepArgument`s of the step function.\n",
        "- Execution environment: Dependencies required for the step to run.\n",
        "\n",
        "In this case, besides our custom steps above, we can also simply utilize Zendikon-provided ones such as `ZendikonAutoMLStep` and `ZendikonAutoMLInferenceStep` to leverage AutoML."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.reusable_steps.automl_step import ZendikonAutoMLStep\n",
        "from zendikon.pipelines.reusable_steps.automl_inference_step import ZendikonAutoMLInferenceStep\n",
        "\n",
        "steps_info = [\n",
        "    PipelineStepInfo(PreprocessStep, \n",
        "        StepConfig(\"Select relevant columns\", inputs=[\"beer_input_dataset\"], outputs=[\"beer_processed_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(DataSplittingStep, \n",
        "        StepConfig(\"Splitting data to train and validation\", inputs=[\"beer_processed_dataset\"], outputs=[\"beer_train_dataset\", \"beer_valid_dataset\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLStep, \n",
        "        StepConfig(\"AutoML training\", inputs=[\"beer_train_dataset\"], outputs=[\"models_info\", \"best_model\"], conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "    PipelineStepInfo(ZendikonAutoMLInferenceStep, \n",
        "        StepConfig(\"AutoML inferencing with best model\", inputs=[\"beer_valid_dataset\", \"best_model\"], outputs=[\"best_model_predicted\"], arguments={\"target_column\": \"BeerProduction\"}, conda_dependencies_file=\"./conda_dependencies.yml\")),\n",
        "]\n",
        "\n",
        "steps_info"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "[<zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f07746c36a0>,\n <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f0773bcdd00>,\n <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f0773ecad60>,\n <zendikon.pipelines._pipeline_step_info.PipelineStepInfo at 0x7f0794586280>]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1647441491589
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running our pipeline on AML\r\n",
        "\r\n",
        "From this point on, we will require an AML workspace to work with. The details can be specified in `config.json`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Setup\n",
        "\n",
        "We begin by using the AML SDK to establish the AML workspace, experiment and compute target we will be utilizing. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, ComputeTarget, Dataset, Environment\n",
        "\n",
        "# AML Setup\n",
        "workspace = Workspace.from_config()\n",
        "print('Workspace name: ' + workspace.name,\n",
        "      'Azure region: ' + workspace.location,\n",
        "      'Subscription id: ' + workspace.subscription_id,\n",
        "      'Resource group: ' + workspace.resource_group, sep='\\n')\n",
        "\n",
        "experiment_name = \"reusable_pipeline_time_series_forecasting\"\n",
        "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
        "compute_target = ComputeTarget(workspace=workspace, name=\"zendikon-cpu-f4\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: zendikon-test\nAzure region: eastus2\nSubscription id: 6f83e421-6b03-4154-9df4-fc8739806b66\nResource group: zendikon\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1647441171427
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zendikon Package\n",
        "Ensure that the `zendikon` package is available in: \n",
        "\n",
        "1. The current environment (notebook) we are currently in. If using from JupyterLab, kernel `zendikon-env` should already have this set up.\n",
        "2. The pipeline's environment once it is submitted to AML. To do so, execute the following and update the dependency link in `conda_dependencies.yml`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this wheel URL in conda_dependencies.yml\n",
        "whl_url = Environment.add_private_pip_wheel(workspace, \"./zendikon-1.8.8.post12-py3-none-any.whl\", exist_ok=True)\n",
        "whl_url"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "'https://zendikontest6824921913.blob.core.windows.net/azureml/Environment/azureml-private-packages/zendikon-1.8.8.post12-py3-none-any.whl'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1647441177667
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing dataset for pipeline\n",
        "Registered datasets in AML are used as input datasets to Zendikon pipelines. We can achieve this in several ways:\n",
        "\n",
        "1. Use [AML Studio (UI)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-connect-data-ui?tabs=credential) to use an existing external datastore and register datasets from it.\n",
        "2. The same can be achieved with the [AML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets).\n",
        "\n",
        "However for simplicity, we directly upload and register the small dataset we have loaded in this example with the SDK.\n",
        "Only do this step if you decide to register the dataset via code instead of using the UI."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = workspace.get_default_datastore()\n",
        "\n",
        "# Register and upload the entire beer dataset to the workspace\n",
        "input_dataset = Dataset.Tabular.register_pandas_dataframe(input_df, datastore, \"beer_input_dataset\", show_progress=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/14208358-220a-4952-9845-11857f1edb2f/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1647441177094
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the forecasting parameters to be used in the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "target_column_name = \"BeerProduction\"\n",
        "time_column_name = \"DATE\"\n",
        "forecast_horizon = 12\n",
        "freq = \"MS\"\n",
        "\n",
        "# Forecasting Parameters\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name=time_column_name,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    freq=freq,  # Set the forecast frequency to be monthly (start of the month)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1647441177360
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline instance\n",
        "\n",
        "primary_metric=“NormRMSE” (normalized root mean squared error, by default). In order to change the primary metric, specify the parameter primary_metric when calling TimeSeriesForecastingPipeline.from_default_settings.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zendikon.pipelines.time_series.forecasting import TimeSeriesForecastingPipeline\n",
        "\n",
        "pipeline = TimeSeriesForecastingPipeline.from_default_automl_config(\n",
        "    input_datasets=[],\n",
        "    forecasting_parameters=forecasting_parameters,\n",
        "    label_column_name=target_column_name,\n",
        "    compute_targets=[compute_target],\n",
        "    steps_info=steps_info)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nWarning: The following datasets (['beer_input_dataset']) are not the output(s) of any step in your pipeline and are assumed to be registered in your pipeline workspace.\nIf this is not your intention, we recommend interrupting the command using Ctrl + c and check your pipeline config.\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1647441492833
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit Pipeline\n",
        "\n",
        "The pipeline will now execute remotely on our specified compute target, and we can track the progress in AML Studio with the generated link below:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.submit(experiment, wait_for_completion=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step Select relevant columns [79a5e91c][0f8aa93b-3cd7-4b7f-996f-e7a170ea6bc5], (This step is eligible to reuse a previous run's output)Created step Splitting data to train and validation [30bdccc9][71c7a0e8-2c3b-499f-87f6-fc4fefa5a086], (This step is eligible to reuse a previous run's output)\n\nCreated step AutoML training [05efd96a][72ec4c8d-7669-4cfe-a190-d39f713bd3ed], (This step will run and generate new outputs)\nCreated step AutoML inferencing with best model [bab490de][1ec7a7fb-d154-486d-9dee-a2dd2d566df3], (This step will run and generate new outputs)\nSubmitted PipelineRun 6a5fe01c-6c92-44cd-953c-23e5b166775a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/6a5fe01c-6c92-44cd-953c-23e5b166775a?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>reusable_pipeline_time_series_forecasting</td><td>6a5fe01c-6c92-44cd-953c-23e5b166775a</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/6a5fe01c-6c92-44cd-953c-23e5b166775a?wsid=/subscriptions/6f83e421-6b03-4154-9df4-fc8739806b66/resourcegroups/zendikon/workspaces/zendikon-test&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: reusable_pipeline_time_series_forecasting,\nId: 6a5fe01c-6c92-44cd-953c-23e5b166775a,\nType: azureml.PipelineRun,\nStatus: Preparing)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1647441500559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get results"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
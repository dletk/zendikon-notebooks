{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Attention: AML Workspace & Auth\n",
        "> When connecting to the AML workspace, you will need to look out for the device code to do an in-browser interactive authentication.  \n",
        "The first section may not require it, but we encourage you to have an AML workspace ready for the full notebook.\n",
        "\n",
        "# Load our dataset\n",
        "\n",
        "In this example, we will be using a small example dataset which contains information about New York city energy demand across time.\n",
        "The dataset is already split into training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timeStamp</th>\n",
              "      <th>demand</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-01-01 00:00:00</td>\n",
              "      <td>4937.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-01-01 01:00:00</td>\n",
              "      <td>4752.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-01-01 02:00:00</td>\n",
              "      <td>4542.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-01-01 03:00:00</td>\n",
              "      <td>4357.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-01-01 04:00:00</td>\n",
              "      <td>4275.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             timeStamp  demand  precip   temp\n",
              "0  2012-01-01 00:00:00  4937.5     0.0  46.13\n",
              "1  2012-01-01 01:00:00  4752.1     0.0  45.89\n",
              "2  2012-01-01 02:00:00  4542.6     0.0  45.04\n",
              "3  2012-01-01 03:00:00  4357.7     0.0  45.03\n",
              "4  2012-01-01 04:00:00  4275.5     0.0  42.61"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = os.path.join(\"./data\")\n",
        "\n",
        "training_data = pd.read_csv(os.path.join(data_dir, \"nyc_data_train.csv\"), parse_dates=True)\n",
        "training_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timeStamp</th>\n",
              "      <th>demand</th>\n",
              "      <th>precip</th>\n",
              "      <th>temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-25 14:00:00</td>\n",
              "      <td>10406.970</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>88.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-25 15:00:00</td>\n",
              "      <td>10543.540</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>90.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-25 16:00:00</td>\n",
              "      <td>10483.260</td>\n",
              "      <td>0.0091</td>\n",
              "      <td>89.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-25 17:00:00</td>\n",
              "      <td>10087.470</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>85.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-25 18:00:00</td>\n",
              "      <td>9593.675</td>\n",
              "      <td>0.3383</td>\n",
              "      <td>83.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             timeStamp     demand  precip   temp\n",
              "0  2016-07-25 14:00:00  10406.970  0.0000  88.87\n",
              "1  2016-07-25 15:00:00  10543.540  0.0000  90.77\n",
              "2  2016-07-25 16:00:00  10483.260  0.0091  89.90\n",
              "3  2016-07-25 17:00:00  10087.470  0.0000  85.11\n",
              "4  2016-07-25 18:00:00   9593.675  0.3383  83.70"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_data = pd.read_csv(os.path.join(data_dir, \"nyc_data_validation.csv\"), parse_dates=True)\n",
        "validation_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Running our pipeline on AML\n",
        "\n",
        "From this point on, we will require an AML workspace to work with.\n",
        "For more details on creating an AML workspace, see [Quickstart: Create workspace resources you need to get started with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources)  \n",
        "\n",
        "**Replace the placeholder AML workspace details below with the workspace you have access to.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Experiment Setup\n",
        "\n",
        "We begin by using the AML SDK to establish the AML workspace, experiment and compute target we will be utilizing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1647441171427
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: zendikon-test\n",
            "Subscription id: 7df29d08-a878-4f14-8044-00033608a1db\n",
            "Resource group: zendikon-test\n",
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Workspace, Experiment, ComputeTarget, Dataset, Environment\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Replace these with your own AML workspace details\n",
        "subscription_id = \"<SUBSCRIPTION ID HERE>\"\n",
        "resource_group = \"<RESOURCE GROUP HERE>\"\n",
        "workspace_name = \"<AML WORKSPACE NAME HERE>\"\n",
        "\n",
        "# Choose a name for your CPU cluster, existing ones can be used.\n",
        "cpu_cluster_name = \"zendikon-cpu-ds3\"\n",
        "\n",
        "# AML Setup\n",
        "workspace = Workspace(\n",
        "      subscription_id=subscription_id,\n",
        "      resource_group=resource_group,\n",
        "      workspace_name=workspace_name\n",
        ")\n",
        "print('Workspace name: ' + workspace.name,\n",
        "      'Subscription id: ' + workspace.subscription_id,\n",
        "      'Resource group: ' + workspace.resource_group, sep='\\n')\n",
        "\n",
        "experiment_name = \"reusable_pipeline_time_series_forecasting\"\n",
        "experiment = Experiment(workspace=workspace, name=experiment_name)\n",
        "\n",
        "# Verify that the cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=workspace, name=cpu_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4, \n",
        "                                                           idle_seconds_before_scaledown=2400)\n",
        "    compute_target = ComputeTarget.create(workspace, cpu_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Preparing dataset for pipeline\n",
        "Registered datasets in AML are used as input datasets to Zendikon pipelines. We can achieve this in several ways:\n",
        "\n",
        "1. Use [AML Studio (UI)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-connect-data-ui?tabs=credential) to use an existing external datastore and register datasets from it.\n",
        "2. The same can be achieved with the [AML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets).\n",
        "\n",
        "However for simplicity, we directly upload and register the small dataset we have loaded in this example with the SDK.\n",
        "Only do this step if you decide to register the dataset via code instead of using the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1647441177094
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating arguments.\n",
            "Arguments validated.\n",
            "Successfully obtained datastore reference and path.\n",
            "Uploading file to managed-dataset/c94f36f0-abc3-40af-a47c-1214421d2bee/\n",
            "Successfully uploaded file to datastore.\n",
            "Creating and registering a new dataset.\n",
            "Successfully created and registered a new dataset.\n",
            "Validating arguments.\n",
            "Arguments validated.\n",
            "Successfully obtained datastore reference and path.\n",
            "Uploading file to managed-dataset/4a393720-eb46-41e5-b4a3-35e4db2863b3/\n",
            "Successfully uploaded file to datastore.\n",
            "Creating and registering a new dataset.\n",
            "Successfully created and registered a new dataset.\n"
          ]
        }
      ],
      "source": [
        "datastore = workspace.get_default_datastore()\n",
        "training_dataset_name = \"nyc_energy_training_data\"\n",
        "validation_dataset_name = \"nyc_energy_validation_data\"\n",
        "\n",
        "# Register and upload the entire beer dataset to the workspace\n",
        "training_dataset = Dataset.Tabular.register_pandas_dataframe(training_data, datastore, name=training_dataset_name, show_progress=True)\n",
        "validation_dataset = Dataset.Tabular.register_pandas_dataframe(validation_data, datastore, name=validation_dataset_name, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prepare the forecasting parameters to be used in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1647441177360
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "target_column_name = \"demand\"\n",
        "time_column_name = \"timeStamp\"\n",
        "forecast_horizon = 48\n",
        "freq = \"H\"\n",
        "\n",
        "# Forecasting Parameters\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name=time_column_name,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    freq=freq,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create the pipeline instance\n",
        "\n",
        "primary_metric=“NormRMSE” (normalized root mean squared error, by default). In order to change the primary metric, specify the parameter primary_metric when calling TimeSeriesForecastingPipeline.from_default_settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1647441492833
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Warning: The following datasets (['nyc_energy_validation_data']) are not the output(s) of any step in your pipeline and are assumed to be registered in your pipeline workspace.\n",
            "If this is not your intention, we recommend interrupting the command using Ctrl + c and check your pipeline config.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from zendikon.pipelines.time_series.forecasting import TimeSeriesForecastingPipeline\n",
        "\n",
        "pipeline = TimeSeriesForecastingPipeline.from_default_settings(\n",
        "    training_dataset=training_dataset_name,\n",
        "    validation_dataset=validation_dataset_name,\n",
        "    forecasting_parameters=forecasting_parameters,\n",
        "    label_column_name=target_column_name,\n",
        "    compute_targets=[compute_target])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Submit Pipeline\n",
        "In order to submit the pipeline, in the first submission, you will need to specify `add_zendikon_feed=True` and `personal_access_token`. This will allow Zendikon package to be installed from Zendikon feed to your pipeline when the pipeline is running on your AML workspace. This only need to be done once! After the first submission, you can specify `add_zendikon_feed=False` (default setting) and leave `personal_access_token` to be `None`.\n",
        "\n",
        "The pipeline will now execute remotely on our specified compute target, and we can track the progress in AML Studio with the generated link below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1647441500559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step zendikon_automl [b54fe52d][88cfd81c-29d4-4820-8520-cc458d5f6614], (This step will run and generate new outputs)\n",
            "Created step zendikon AutoML evaluation [1c94dfda][0b401bd3-4593-4aca-ab07-5faf064b8bd4], (This step will run and generate new outputs)\n",
            "Created step Models metrics summary [4c79941e][7393a29c-efbe-4bc2-958f-22f259999571], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun 217d1684-69f3-4d8c-9e43-c649655937e6\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/217d1684-69f3-4d8c-9e43-c649655937e6?wsid=/subscriptions/7df29d08-a878-4f14-8044-00033608a1db/resourcegroups/zendikon-test/workspaces/zendikon-test&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>reusable_pipeline_time_series_forecasting</td><td>217d1684-69f3-4d8c-9e43-c649655937e6</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/runs/217d1684-69f3-4d8c-9e43-c649655937e6?wsid=/subscriptions/7df29d08-a878-4f14-8044-00033608a1db/resourcegroups/zendikon-test/workspaces/zendikon-test&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: reusable_pipeline_time_series_forecasting,\n",
              "Id: 217d1684-69f3-4d8c-9e43-c649655937e6,\n",
              "Type: azureml.PipelineRun,\n",
              "Status: Running)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.submit(experiment, wait_for_completion=False, add_zendikon_feed=True, personal_access_token=\"<YOUR PAT>\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('zendikon37')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f518ce44f98f045a778dd62beacc4c6bd6975e54f1df8536bc21feb807c48a8e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
